## CVPR 2017

### transfer learning

#### [Borrowing Treasures From the Wealthy: Deep Transfer Learning Through Selective Joint Fine-Tuning, Weifeng Ge, Yizhou Yu](https://arxiv.org/pdf/1702.08690.pdf)
这篇文章是做迁移学习，当源任务和目标任务类似时，如何将源任务的大数据集用到目标任务的小数据集中，以更好补充数据，实现目标任务。
source源任务——大规模数据集
target目标任务——小规模数据集，
数据集是图像，源任务和目标任务共享卷积层特征，从大规模数据中找出低层特征和小规模数据集中低层特征类似(histogram-based, 特征的KL散度作相似度度量)的图像，一起加入训练。一方面两个任务共享卷积层参数防止过拟合，另一方面，针对于目标任务选择了更多类似图片，这低层卷积层核函数提取特征对于目标任务更具有鲁棒性。
为什么选低层特征，是因为我们认为这些低层特征决定了高层特征，而高层特征可能具有不同的语义信息。（我想到一个例子，比方说，源任务和目标任务低层特征具有某种边缘特性，那么我们去找具有类似边缘特性的图像扩充目标任务数据集，可以减少其他和目标任务无关的特征影响，保持新增样本在目标任务中发挥作用）


### cross
#### [Split-BrainAutoencoders:UnsupervisedLearningby Cross-Channel Prediction, Richard Zhang, Phillip Isola, Alexei A. Efros](https://arxiv.org/pdf/1611.09842.pdf)
这篇文章立足于传统自编码技术，将神经网络分割位两个子网络，实现输入数据不同通道间的转换。
这个工作是无监督学习
